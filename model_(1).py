# -*- coding: utf-8 -*-
"""model (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cdQAYi6F3GBboHfQeQOirzq_C2M-EGrT
"""

import numpy as np
import pandas as pd
import os
import pickle
from time import sleep

import numpy as np
import os
import pandas as pd
import time
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn import metrics
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier , GradientBoostingClassifier , AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

df1 = pd.read_csv("Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv")
df2=pd.read_csv("Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv")
df3=pd.read_csv("Friday-WorkingHours-Morning.pcap_ISCX.csv")
df4=pd.read_csv("Monday-WorkingHours.pcap_ISCX.csv")
df5=pd.read_csv("Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv")
df6=pd.read_csv("Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv")
df7=pd.read_csv("Tuesday-WorkingHours.pcap_ISCX.csv")
df8=pd.read_csv("Wednesday-workingHours.pcap_ISCX.csv")

nRowsRead = None
df = pd.concat([df1,df2])
del df1,df2
df = pd.concat([df,df3])
del df3
df = pd.concat([df,df4])
del df4
df = pd.concat([df,df5])
del df5
df = pd.concat([df,df6])
del df6
df = pd.concat([df,df7])
del df7
df = pd.concat([df,df8])
del df8
df.shape

df[' Label'].value_counts()

plt.figure(figsize=(12, 6))
plot = sns.countplot(y=df[' Label'])
plt.xscale('log')
fig = plot.get_figure()
fig.savefig('img1.png')

df.duplicated().sum()

df.shape

df =  df.drop_duplicates(keep="first")

df.duplicated().sum()

df.shape

df.isnull().sum().sort_values(ascending = False)

df.dropna(inplace=True)

df.shape

df.isnull().sum().sort_values(ascending = False)

df.shape

df=df.groupby(' Label').filter(lambda x:len(x)>10000)
df[' Label'].value_counts()

integer = []
f = []
for i in df.columns[:-1]:
    if df[i].dtype == "int64": integer.append(i)
    else : f.append(i)

df[integer] = df[integer].astype("int32")
df[f] = df[f].astype("float32")

df.shape

df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]
# df.reset_index(drop=True,inplace=True)

df.shape

def correlation(dataset, threshold):
    col_corr = set()
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
              colname = corr_matrix.columns[i]
              col_corr.add(colname)
    return col_corr

corr_features = correlation(df, 0.85)
corr_features

df.drop(corr_features,axis=1,inplace=True)

y = df[' Label']
x = df.drop([' Label'],axis=1)

x.head()

y.head()

rus = RandomUnderSampler(random_state=0)
rus.fit(x, y)
Xn, yn = rus.fit_resample(x, y)
# Xn.value_counts()

Xn.head()
Xn.shape

cols = list(Xn.columns)
for col in cols:
    Xn[col] = stats.zscore(Xn[col])

from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
X_train, X_test, Y_train, Y_test = train_test_split(Xn,yn,test_size=0.30,random_state=0)

print(np.any(np.isnan(X_train)))
print(np.all(np.isfinite(X_train)))